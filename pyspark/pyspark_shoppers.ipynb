{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "In the past, my only experience with Spark was through Databricks in the 'Introduction to Spark' [EdX class](https://www.edx.org/course/introduction-apache-spark-uc-berkeleyx-cs105x). From my experience there, I found that the databricks environment was similar to that of Jupyter notebooks, and the syntax and operations of pyspark <strong>dataframes</strong>  were similar to what I'd use in SQL and Pandas. Thus, I decided that it was a good time to test it out locally. In this walkthrough, I hope to be able to load some Big Data (22 gigs) and see if I can perform some standard operations, and perhaps even some simple machine learning using MLLib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I moved my 22 gig csv file (courtesy of [Kaggle](https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data?transactions.csv.gz) into my spark folder. Recall that in order to read a csv file with spark, you need to call it with a flag from terminal (even for notebook). In this case, it would be the following:\n",
    "\n",
    "bin/pyspark --packages com.databricks:spark-csv_2.10:1.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x10998d8d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's doublecheck that SparkContext is loaded on our machine\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to using the flag from terminal, I also have to specifiy the databricks package from the options function in order to read the csv file properly into a pyspark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data= sqlContext.read.format('com.databricks.spark.csv').options(\n",
    "    header='true', inferschema='true').load('kaggle_shoppers_transactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+--------+----------+-----+----------+-----------+--------------+----------------+--------------+\n",
      "|   id|chain|dept|category|   company|brand|      date|productsize|productmeasure|purchasequantity|purchaseamount|\n",
      "+-----+-----+----+--------+----------+-----+----------+-----------+--------------+----------------+--------------+\n",
      "|86246|  205|   7|     707|1078778070|12564|2012-03-02|       12.0|            OZ|               1|          7.59|\n",
      "|86246|  205|  63|    6319| 107654575|17876|2012-03-02|       64.0|            OZ|               1|          1.59|\n",
      "|86246|  205|  97|    9753|1022027929|    0|2012-03-02|        1.0|            CT|               1|          5.99|\n",
      "|86246|  205|  25|    2509| 107996777|31373|2012-03-02|       16.0|            OZ|               1|          1.99|\n",
      "|86246|  205|  55|    5555| 107684070|32094|2012-03-02|       16.0|            OZ|               2|         10.38|\n",
      "|86246|  205|  97|    9753|1021015020|    0|2012-03-02|        1.0|            CT|               1|           7.8|\n",
      "|86246|  205|  99|    9909| 104538848|15343|2012-03-02|       16.0|            OZ|               1|          2.49|\n",
      "|86246|  205|  59|    5907| 102900020| 2012|2012-03-02|       16.0|            OZ|               1|          1.39|\n",
      "|86246|  205|   9|     921| 101128414| 9209|2012-03-02|        4.0|            OZ|               2|           1.5|\n",
      "|86246|  205|  73|    7344|1068142161|20285|2012-03-02|        8.0|            CT|               1|          5.79|\n",
      "|86246|  205|  41|    4107| 104113040|28204|2012-03-02|       14.5|            OZ|               1|          0.59|\n",
      "|86246|  205|  21|    2106| 105100050|27873|2012-03-02|       64.0|            OZ|               1|          3.29|\n",
      "|86246|  205|   8|     814| 102840020|18584|2012-03-02|       15.5|            OZ|               1|          3.29|\n",
      "|86246|  205|  91|    9122| 108200080| 2911|2012-03-02|       10.0|            OZ|               1|          1.99|\n",
      "|86246|  205|  41|    4120| 101116616|15266|2012-03-02|        6.0|            OZ|               1|          0.89|\n",
      "|86246|  205|  63|    6315| 107996777|31373|2012-03-02|       64.0|            OZ|               1|          3.59|\n",
      "|86246|  205|   9|     907| 101410010|13791|2012-03-02|       24.0|            OZ|               1|          3.99|\n",
      "|86246|  205|  97|    9753|1021013323|    0|2012-03-02|        1.0|            CT|               1|          8.87|\n",
      "|86246|  205|  45|    4509|1082650484|59628|2012-03-02|       16.0|            OZ|               1|          4.99|\n",
      "|86246|  205|  26|    2630| 103700030|14647|2012-03-02|       56.0|            CT|               1|           1.0|\n",
      "+-----+-----+----+--------+----------+-----+----------+-----------+--------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet! It took about 5 min to 'load', but I was able to get it in there :) Let's check that the object is a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
