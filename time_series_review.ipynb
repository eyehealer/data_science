{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Python to process data that's too large to fit into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I generated some sample data into a database using [Mockaroo](https://www.mockaroo.com/) that includes bg values for different users throughout the day. \n",
    "\n",
    "Although the data is only 600 rows, I'll use this opportunity to process it chunkwise in python as if it were data that could not fit in memory. Let's get a quick look at the data first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Database table chunks with pandas](#Database table chunks with pandas)\n",
    "\n",
    "[Buffering with a CSV file and Numpy](#Buffering with a CSV file and Numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database table chunks with pandas\n",
    "\n",
    "<a id='Database table chunks with pandas'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bgVal</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>230</td>\n",
       "      <td>15:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>19:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>347</td>\n",
       "      <td>8:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  bgVal   time\n",
       "0   1    230  15:52\n",
       "1   2     80  19:22\n",
       "2   3    347   8:13"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "#Connect to our database\n",
    "con= sqlite3.connect('bgTable.db')\n",
    "\n",
    "#Read the first few lines from our table\n",
    "table0= pd.read_sql_query('SELECT * FROM bgTable LIMIT 3', con)\n",
    "table0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can see our table, we'll need to define the query that we'd like to apply to the table. We want to add a new column to our data that labels our row based on the bgValues value. It can fall into three groups: 1) Less than or equal to 80 2) between 81 and 250 and 3) Greater than 250. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#binItems query has nested 'case when' statements to create a new\n",
    "#column called 'bin' that includes a label for each row depending\n",
    "#on the value of the bgVal column\n",
    "binItems= '''\n",
    "SELECT *, \n",
    "CASE WHEN bgVal <=80 THEN 'below'  \n",
    "     ELSE CASE WHEN bgVal <=250 THEN 'in_range'  \n",
    "          ELSE CASE WHEN bgVal >250 THEN 'above' \n",
    "          END \n",
    "     END\n",
    "END AS 'bin'\n",
    "FROM bgTable\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the query, we'll apply it to bgTable. This will add an extra column ('bin') for the bin each row belongs to ('below','in_range', and 'above'). Again, we'll perform this operation in chunks (since we're assuming that the table is too large to load into memory). \n",
    "\n",
    "The code in the next cell does the following: 1) loads a chunk of our table from our database 3 rows at a time, 2) performs the 'binItems' query on each chunk and 3) appends the resulting chunks to a new table called 'bgTableBinned' that has a new 'bin' column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x111fa0d50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Changing this to query from sqlite directly \n",
    "#instead of through pandas, since that crashes after creating table\n",
    "c= con.cursor()\n",
    "c.execute('create table bgTableBinned2(id INTEGER, bgVal VARCHAR, time VARCHAR, bin VARCHAR)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now we'll read, process, write chunks iteratively like before\n",
    "#except that now table we'll append to already exists\n",
    "\n",
    "for query_chunk in pd.read_sql_query(binItems, con, chunksize=3):\n",
    "    #print query_chunk \n",
    "    query_chunk.to_sql('bgTableBinned2',con, if_exists= 'append', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, so now have a small victory: we no longer have the previous cell crashing (before we had to run same cell twice and had 607 rows appended total). However, we still get 604 lines appended instead of 600. If you look at the id's in the first rows you see: 1,2,3,4,1,2,3...600. This means it got to row 4 then it reset (without crashing) and starts over and executes remaining appends without problem. \n",
    "\n",
    "Could this be a bug in the pd.to_sql method? I checked with the data engineers and they couldn't understand why those four rows were interrupted and they couldn't understand why a) it reset and b) why it didn't result in an error given that reset. \n",
    "\n",
    "I'll keep an eye on this but I learned a lot about doing things this way. In real life, if the problem was this basic and data was too large, I'd just do it as a sql query. For more complicated issues that require a custom lambda function in pandas, this could be a problem though (may try this in R to see if it's easier there)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save and close connection to database\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bgVal</th>\n",
       "      <th>time</th>\n",
       "      <th>bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>230</td>\n",
       "      <td>15:52</td>\n",
       "      <td>in_range</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>19:22</td>\n",
       "      <td>below</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>347</td>\n",
       "      <td>8:13</td>\n",
       "      <td>above</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id bgVal   time       bin\n",
       "0   1   230  15:52  in_range\n",
       "1   2    80  19:22     below\n",
       "2   3   347   8:13     above"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reopen connection and read new table\n",
    "\n",
    "#Connect to our database with new table\n",
    "conNew= sqlite3.connect('bgTable.db')\n",
    "\n",
    "#Read the first few lines from our new table\n",
    "newTable= pd.read_sql_query('SELECT * FROM bgTableBinned2 LIMIT 3', conNew)\n",
    "newTable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conNew.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buffering with a CSV file and Numpy\n",
    "\n",
    "<a id='Buffering with a CSV file and Numpy'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a csv file of the data in bgTable in order to perform the same operations with this file format. I could so something similar to what I did with the db file using pandas, but I'll start by avoiding external modules.\n",
    "\n",
    "The key argument that we'll use is the 'buffering' argument for the 'open' function. Before that though, let's define the function that will bin the values into our three bins once more and test the operations on a separate table before we insert it into the chunk loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bgVal</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>230</td>\n",
       "      <td>15:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>19:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>347</td>\n",
       "      <td>8:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  bgVal   time\n",
       "0   1    230  15:52\n",
       "1   2     80  19:22\n",
       "2   3    347   8:13"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's try operating on the sample table from the first line in our notebook\n",
    "table0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 230, u'15:52'],\n",
       "       [2, 80, u'19:22'],\n",
       "       [3, 347, u'8:13']], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We'll need to import numpy and turn this dataframe into an array\n",
    "#Let's verify that we have a three column array\n",
    "import numpy as np\n",
    "\n",
    "arrayTable= np.array(table0)\n",
    "arrayTable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binFxn(myArray):\n",
    "    '''This function will be passed and applied to every row in \n",
    "    my chunk (to avoid iterating through a for loop) and will\n",
    "    create create a new column that labels a row based on the\n",
    "    value in its second column as below, in_range, or above'''\n",
    "    if  myArray[1] <= 80: \n",
    "        return 'below'\n",
    "    elif myArray[1] <= 250 and myArray[1] > 80:\n",
    "        return 'in_range'\n",
    "    elif myArray[1] >= 250:\n",
    "        return 'above'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We'll apply the binFxn to each row in our table and \n",
    "#the output will be a new column that we'll call newArray\n",
    "newArray= np.apply_along_axis(binFxn, 1, arrayTable)\n",
    "newArray= newArray.reshape(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 230, u'15:52', 'in_range'],\n",
       "       [2, 80, u'19:22', 'below'],\n",
       "       [3, 347, u'8:13', 'above']], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We'll append this new array to our origina array\n",
    "#that should give us a 4 column table\n",
    "np.append(arrayTable, newArray, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Seems to be doing what we want it to do. Now we just need to perform this action on every chunk, and then write/append that to a new file. Let's try implementing that on a version of bgTable that comes in csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'file' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b2e7dd2de1ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#buffering=1 mb is the smallest unit we can choose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bgTable.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyFile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0marrayTable\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mnewArray\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinFxn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrayTable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'file' has no len()"
     ]
    }
   ],
   "source": [
    "#Initialize new file to append the new table data into\n",
    "newTable= open('bgTableNew.csv','a')\n",
    "\n",
    "#We'll use the buffering argument to limit the chunk size\n",
    "#buffering=1 mb is the smallest unit we can choose\n",
    "with open('bgTable.csv', buffering=1) as myFile:\n",
    "    for chunk in myFile:\n",
    "        arrayTable= np.array(chunk)\n",
    "        newArray= np.apply_along_axis(binFxn, 1, arrayTable)\n",
    "        #newArray= newArray.reshape(3,1)\n",
    "        print newArray\n",
    "        #newTable.write(np.append(arrayTable, newArray, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. So performing the steps without considering chunks works ok, but we encounter some trouble when inserting it into the chunk loop. If I print chunk or arrayTable I get the full result (I think it only did one loops since my data is less than the minimum of 1 MB. However, I get an error when I create newArray. \n",
    "\n",
    "I think the problem might be that I'm performing the operations on the first row which is column labels. In the future step I need to find out how to skip that first row. This post looks relevant: http://stackoverflow.com/questions/14257373/skip-the-headers-when-editing-a-csv-file-using-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
