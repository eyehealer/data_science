{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing and Power Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Motivation](#Motivation)\n",
    "\n",
    "[Experimental Design](#Experimental Design)\n",
    "\n",
    "[Hypothesis Testing](#Hypothesis Testing)\n",
    "\n",
    "[Power Analysis](#Power Analysis)\n",
    "\n",
    "[Final Thoughts](#Final Thoughts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "<a id='Motivation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A/B testing is an experimental method used to <strong>measure the effect of a variable on a target response</strong>. For example, many tech companies use this method to test the effect of product features on an outcome such as visits, click-through rate, visits, or other metrics that indicate business success. As a data scientist, the outcome of your <strong>A/B tests can help your clients make data-driven business decisions.</strong> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Design\n",
    "\n",
    "<a id='Experimental Design'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before conducting your A/B test, it's important to understand your system, your data, and potential outcomes. Typically, you'll be testing whether the difference you observe between two conditions is significant enough to make a decision/enact a feature on that action. Here are a few questions you should answer before performing an A/B test:\n",
    "\n",
    "1. What is the metric that I will be measuring? What variables will I be comparing? \n",
    "2. Do I currently have the data to answer this question? Is it continuous or categorical?\n",
    "3. How will I test my hypothesis?\n",
    "4. How much data will be necessary to answer my question? This is called  <strong>Power Analysis</strong>.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "<a id='Hypothesis Testing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we have our data, we need to test it. \n",
    "\n",
    "Calculating the confidence intervals. If we know the means and stdev for our two samples, we can calculate where the boundaries for 95% of the true mean are by using our z-score table. First, look for 0.95 in the table, then find the tenths and then hundredths digits to get the z-score. This z-score essentially tells you how many standard deviations away the 95% boundary is. If we're outside that, then we can say the boundaries for the difference between the means of our two samples, then we can say that with ___% confidence, I can define where the 95% of the true sample mean is \n",
    "\n",
    "See khan academy explanation: https://www.youtube.com/watch?v=yQsCMnz9wO8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous Data:     <strong>Continuous</strong>: How is your dependent variable distributed? What's your range of values? Do they make sense? \n",
    "\n",
    "a) Continuous: If normally distributed: paired (before and after) or unpaired (one condition vs 2nd condition) t-tests. If NOT normally distributed, t-tests may still work, but you'll likely need very large datasets. A better option could be to use equivalent tests that don't assume normality, and compare the medians instead of means. Be cautious about performing t-test on log-transformed data: means you're comparing may no longer reflect differences in means of original data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical Data: Is it balanced? Will you have to use proportions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Analysis\n",
    "\n",
    "<a id='Power Analysis'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power analysis is a method in experimental design used to calculate the sample size needed to detect an effect at a certain confidence level. When comparing the <strong>difference of means</strong> for a control and an experimental group in an A/B test, we calculate the sample size as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\ n = \\frac{2*sigma^2*(beta + alpha)^2}{effect^2} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Where n is the sample size, sigma is the standard deviation, beta is the power (1-type II error or prob. of finding an effect that is there), alpha is the type I error, and effect is the difference in means of the two groups you expect to see in your measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Statistical Power refers to the probability that you'll accurately detect True Positives while avoiding False Positives of significant differences. http://meera.snre.umich.edu/power-analysis-statistical-significance-effect-size It makes sense that with more data you'll be able to differentiate appropriately, but how much is enough? It is recommended that you have a power of 0.8 or above. \n",
    "Power (sensitivity) has an inverse trade off with size. \n",
    "b) Applying it: Tools to calculate it - baseline conversion rate, practical significance level (absolute or relative)\n",
    "\n",
    "p-values: http://fivethirtyeight.com/features/statisticians-found-one-thing-they-can-agree-on-its-time-to-stop-misusing-p-values/ \n",
    "\n",
    "Effect Size - A metric that allows you to compare the results of significance of multiple tests on the same scale. (meanGroup1-meanGroup2/stdev(eitherGroup)) . 0.3-0.5 are considered moderate, and anything above is considered large difference effect. \n",
    "\n",
    "Since you can't calculate this until after experiment, for calculating power, it is suggested you use 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "\n",
    "<a id='Final Thoughts'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define question, and a hypothesis: eg. changing the color of my button on a page will increase user engagement.\n",
    "\n",
    "Define metric: eg. how do you measure 'user engagement'? Keep it practical (eg. clicks instead of certification). Rates or probability are common examples. Rates used to measure usability of site, probability used to measure total impact.\n",
    "\n",
    "<strong>Business Acumen</strong>\n",
    "*Understand your product (sensitive to seasonality?)\n",
    "*Understand your users\n",
    "*Understand results\n",
    "**Power analysis \n",
    "**Implications of results\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binomial - two possible outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Final Observations\n",
    "*Understand the quality of data that you're collecting (shitty data gives shitty results). \n",
    "*Record the conditions of your experiments thoroughly (eg. metadata). Chances are you'll have to repeat the experiment, but proper design of your experiment will save time from having to repeat it again later. \n",
    "*Stay focused on answering question at hand.\n",
    "*When testing the outcomes of a result, preferably run the experiment simultaneously if possible. Time variations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can it be used for and what will it not cover. 'Features, changes to UI, speed of response from company'. Isn't good at testing out new experiences, don't want confounding variables (eg. new effect). \n",
    "\n",
    "Requirements: Need to randomly sample users. Need to be practical (eg. time, resources).\n",
    "\n",
    "Target: Consistent responses from groups, see if there's a significant behavior change. For tech A/B testing we have larger groups, but we don't know specifics about users in experiment. Want repeatable results. \n",
    "\n",
    "Things to watch out for: confounding effects (seasonality), biases (non-randomized sampling), testing for wrong question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Statistical vs Substantive Significance\n",
    "\n",
    "How much of a difference is it worth it to act based on that. Repeatability is key. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many companies use software that make it easier to run these experiments. They have indicators about when to finish experiment (once your p-value has surpassed threshold), but one needs to be careful about doing it correctly (eg. ending the experiment too early). http://blog.sumall.com/journal/optimizely-got-me-fired.html\n",
    "\n",
    "Don't be afraid to run an experiment more than twice, there's a good chance you'll have to even if you've standardized your conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
